{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T00:44:12.103646Z",
     "start_time": "2024-11-23T00:44:12.100296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ],
   "id": "687a6eef1880ff24",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-25T18:58:40.116499Z",
     "start_time": "2024-11-25T18:58:37.650840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "import re\n",
    "import pandas\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# hack because venv nonsense\n",
    "# model = 'en_core_web_sm'\n",
    "# model = 'en_core_web_md'\n",
    "model = 'en_core_web_lg'\n",
    "model_location = \"..\\\\.venv\\\\Lib\\\\site-packages\\\\\" + model + \"\\\\\" + model + \"-3.8.0\"\n",
    "\n",
    "nlp = spacy.load(model_location) # python -m spacy download en_core_web_lg\n",
    "spacy.info()\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spacy_version': '3.8.2',\n",
       " 'location': 'C:\\\\Users\\\\tbadmin\\\\Documents\\\\projects\\\\machine_learning_musings\\\\.venv\\\\Lib\\\\site-packages\\\\spacy',\n",
       " 'platform': 'Windows-11-10.0.22631-SP0',\n",
       " 'python_version': '3.12.3',\n",
       " 'pipelines': {'en_core_web_lg': '3.8.0',\n",
       "  'en_core_web_md': '3.8.0',\n",
       "  'en_core_web_sm': '3.8.0'}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "id": "7717da458e217594",
    "ExecuteTime": {
     "end_time": "2024-11-25T19:19:11.695614Z",
     "start_time": "2024-11-25T19:19:11.688234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def camel_case_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # tokenize across: python case, java case, nlp (space)\n",
    "    tokens = []\n",
    "    for token in snake_tokenize(string):\n",
    "        for sub_token in space_tokenize(token):\n",
    "            tokens.extend(camel_case_tokenize(sub_token))\n",
    "    # tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "id": "4f700e12fd4994aa",
    "ExecuteTime": {
     "end_time": "2024-11-25T19:19:50.092052Z",
     "start_time": "2024-11-25T19:19:50.081034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replacement_tokens = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def normalize(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in replacement_tokens:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def tokenize_then_normalize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = normalize(tokens)\n",
    "    return tokens\n",
    "\n",
    "def stripHtml (html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', '')\n",
    "    return text"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366",
    "ExecuteTime": {
     "end_time": "2024-11-25T19:19:52.087303Z",
     "start_time": "2024-11-25T19:19:52.075684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_nlp(tokens):\n",
    "    return nlp(\" \".join(tokens))\n",
    "\n",
    "def prepare_df(df, field, chunk_size=100):\n",
    "    \n",
    "    result_prefix = 'tokenized_' + field\n",
    "    df[result_prefix] = None\n",
    "    df['nlp_' + result_prefix] = None\n",
    "\n",
    "    num_chunks = len(df) // chunk_size + int(len(df) % chunk_size != 0)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        print(f'processing chunk {start}...{end}')\n",
    "\n",
    "        chunk = df.iloc[start:end]\n",
    "\n",
    "        df.loc[start:end - 1, result_prefix] = chunk[field].apply(tokenize_then_normalize)\n",
    "        df.loc[start:end - 1, 'nlp_' + result_prefix] = chunk[result_prefix].apply(to_nlp)\n",
    "\n",
    "    return df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-25T19:19:53.823868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_data_dictionary_df():\n",
    "    \"\"\"\n",
    "    Make a fake data dictionary for testing\n",
    "    \"\"\"\n",
    "    dictionary_fields = [\n",
    "        \"POLICY_NUMBERs\",\n",
    "        \"POL_NUMBER\",\n",
    "        # \"policyNumber\",\n",
    "        # \"polNo\",\n",
    "        # \"PolicyNumber\",\n",
    "        \"COVERAGE\",\n",
    "        \"ANNUAL_PREMIUM\"\n",
    "    ]\n",
    "    \n",
    "    dictionary_df = pandas.DataFrame(\n",
    "        dictionary_fields,\n",
    "        columns=[\"field_names\"])\n",
    "    \n",
    "    dictionary_df = prepare_df(dictionary_df, \"field_names\")\n",
    "    return dictionary_df\n",
    "    \n",
    "dictionary_df = get_data_dictionary_df()\n",
    "dictionary_df"
   ],
   "id": "a5eaa75e9188f6f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunk 0...100\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8e97c24098aa98c1",
    "ExecuteTime": {
     "end_time": "2024-11-25T18:59:24.952088Z",
     "start_time": "2024-11-25T18:59:10.443619Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_acord_df():\n",
    "    \"\"\"\n",
    "    Read the glossary data\n",
    "    \"\"\"\n",
    "    acord_df = pandas.read_csv(\n",
    "        'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "        header=0)\n",
    "    \n",
    "    \n",
    "    acord_df.rename(columns={'Glossary Terms': 'glossary'}, inplace=True)\n",
    "    \n",
    "    acord_df = prepare_df(acord_df, \"glossary\") \n",
    "    \n",
    "    return acord_df\n",
    "\n",
    "# ocasionally get windows access violations. they are not being caught, the process just hangs\n",
    "try:\n",
    "    acord_df = get_acord_df()\n",
    "except Exception as e:   \n",
    "    print(e)\n",
    "    \n",
    "acord_df[[ \n",
    "    'glossary', \n",
    "    'tokenized_glossary', \n",
    "    'nlp_tokenized_glossary']]\n"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunk 0...100\n",
      "processing chunk 100...200\n",
      "processing chunk 200...300\n",
      "processing chunk 300...400\n",
      "processing chunk 400...500\n",
      "processing chunk 500...600\n",
      "processing chunk 600...700\n",
      "processing chunk 700...800\n",
      "processing chunk 800...900\n",
      "processing chunk 900...1000\n",
      "processing chunk 1000...1100\n",
      "processing chunk 1100...1200\n",
      "processing chunk 1200...1300\n",
      "processing chunk 1300...1400\n",
      "processing chunk 1400...1500\n",
      "processing chunk 1500...1600\n",
      "processing chunk 1600...1700\n",
      "processing chunk 1700...1800\n",
      "processing chunk 1800...1900\n",
      "processing chunk 1900...2000\n",
      "processing chunk 2000...2100\n",
      "processing chunk 2100...2200\n",
      "processing chunk 2200...2300\n",
      "processing chunk 2300...2400\n",
      "processing chunk 2400...2500\n",
      "processing chunk 2500...2600\n",
      "processing chunk 2600...2700\n",
      "processing chunk 2700...2800\n",
      "processing chunk 2800...2900\n",
      "processing chunk 2900...3000\n",
      "processing chunk 3000...3100\n",
      "processing chunk 3100...3200\n",
      "processing chunk 3200...3300\n",
      "processing chunk 3300...3400\n",
      "processing chunk 3400...3500\n",
      "processing chunk 3500...3600\n",
      "processing chunk 3600...3700\n",
      "processing chunk 3700...3800\n",
      "processing chunk 3800...3900\n",
      "processing chunk 3900...4000\n",
      "processing chunk 4000...4100\n",
      "processing chunk 4100...4200\n",
      "processing chunk 4200...4300\n",
      "processing chunk 4300...4400\n",
      "processing chunk 4400...4500\n",
      "processing chunk 4500...4600\n",
      "processing chunk 4600...4700\n",
      "processing chunk 4700...4800\n",
      "processing chunk 4800...4900\n",
      "processing chunk 4900...5000\n",
      "processing chunk 5000...5100\n",
      "processing chunk 5100...5200\n",
      "processing chunk 5200...5300\n",
      "processing chunk 5300...5400\n",
      "processing chunk 5400...5500\n",
      "processing chunk 5500...5600\n",
      "processing chunk 5600...5700\n",
      "processing chunk 5700...5800\n",
      "processing chunk 5800...5900\n",
      "processing chunk 5900...6000\n",
      "processing chunk 6000...6100\n",
      "processing chunk 6100...6200\n",
      "processing chunk 6200...6300\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Glossary Terms'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 22\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:   \n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28mprint\u001B[39m(e)\n\u001B[1;32m---> 22\u001B[0m \u001B[43macord_df\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mGlossary Terms\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtokenized_glossary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnlp_tokenized_glossary\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\projects\\machine_learning_musings\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   4106\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_iterator(key):\n\u001B[0;32m   4107\u001B[0m         key \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(key)\n\u001B[1;32m-> 4108\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_indexer_strict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcolumns\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m   4110\u001B[0m \u001B[38;5;66;03m# take() does not accept boolean indexers\u001B[39;00m\n\u001B[0;32m   4111\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(indexer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdtype\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\projects\\machine_learning_musings\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001B[0m, in \u001B[0;36mIndex._get_indexer_strict\u001B[1;34m(self, key, axis_name)\u001B[0m\n\u001B[0;32m   6197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   6198\u001B[0m     keyarr, indexer, new_indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reindex_non_unique(keyarr)\n\u001B[1;32m-> 6200\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_raise_if_missing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeyarr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindexer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   6202\u001B[0m keyarr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtake(indexer)\n\u001B[0;32m   6203\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, Index):\n\u001B[0;32m   6204\u001B[0m     \u001B[38;5;66;03m# GH 42790 - Preserve name from an Index\u001B[39;00m\n",
      "File \u001B[1;32m~\\Documents\\projects\\machine_learning_musings\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001B[0m, in \u001B[0;36mIndex._raise_if_missing\u001B[1;34m(self, key, indexer, axis_name)\u001B[0m\n\u001B[0;32m   6249\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNone of [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m] are in the [\u001B[39m\u001B[38;5;132;01m{\u001B[39;00maxis_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   6251\u001B[0m not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(ensure_index(key)[missing_mask\u001B[38;5;241m.\u001B[39mnonzero()[\u001B[38;5;241m0\u001B[39m]]\u001B[38;5;241m.\u001B[39munique())\n\u001B[1;32m-> 6252\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnot_found\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not in index\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mKeyError\u001B[0m: \"['Glossary Terms'] not in index\""
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-25T19:10:50.464060Z",
     "start_time": "2024-11-25T19:10:50.458247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def spacy_similarity(doc1, doc2):\n",
    "    return doc1.similarity(doc2)"
   ],
   "id": "6669cd7a405a3857",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5",
    "ExecuteTime": {
     "end_time": "2024-11-25T19:10:54.192342Z",
     "start_time": "2024-11-25T19:10:54.038702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_matches_df(a_df=dictionary_df, a_field='nlp_tokenized_field_names', b_df=acord_df, b_field='nlp_tokenized_glossary'): \n",
    "\n",
    "    matches = []\n",
    "    \n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, a_encoding in a_df[a_field].items():\n",
    "        best_similarity = float('-inf')\n",
    "        best_match_idx = None\n",
    "    \n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, b_encoding in b_df[b_field].items():\n",
    "            similarity = spacy_similarity(a_encoding, b_encoding)\n",
    "            if similarity > best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "    \n",
    "        # Append the best match for the current dictionary token\n",
    "        matches.append([idx, best_match_idx, best_similarity])\n",
    "        \n",
    "    matches_df = pandas.DataFrame(matches, columns=[\n",
    "        'a_index',\n",
    "        'b_index',\n",
    "        'spacy_similarity'\n",
    "    ])\n",
    "        \n",
    "    return matches_df\n",
    "\n",
    "matches_df = get_matches_df()\n",
    "matches_df"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_12484\\590400431.py:2: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  return doc1.similarity(doc2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   a_index  b_index  spacy_similarity\n",
       "0        0     4811          0.521119\n",
       "1        1     4811          1.000000\n",
       "2        2     1649          1.000000\n",
       "3        3     3021          0.899214"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_index</th>\n",
       "      <th>b_index</th>\n",
       "      <th>spacy_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.521119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3021</td>\n",
       "      <td>0.899214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c",
    "ExecuteTime": {
     "end_time": "2024-11-25T19:18:15.891991Z",
     "start_time": "2024-11-25T19:18:15.853941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_joined(a_df=dictionary_df, a_field='a_index', b_df=acord_df, b_field='b_index'):\n",
    "    # extract index from target (dictionary) and reset index\n",
    "    reset_a_df = (a_df.loc[matches_df['a_index']].reset_index(drop=True))\n",
    "    \n",
    "    # extract index from proposed match (acord) and reset index\n",
    "    reset_b_df = (b_df.loc[matches_df['b_index']].reset_index(drop=True))\n",
    "    \n",
    "    # join target df with matches df\n",
    "    joined_df = reset_a_df.join(reset_b_df)\n",
    "    joined_df['spacy_similarity'] = matches_df['spacy_similarity']\n",
    "    \n",
    "    joined_df['definition'] =  (\n",
    "        joined_df['Definition'].apply(stripHtml))\n",
    "    \n",
    "    joined_df = joined_df[[\n",
    "        'field_names',\n",
    "        'glossary',\n",
    "        'definition',\n",
    "        'spacy_similarity'\n",
    "    ]]\n",
    "    return joined_df\n",
    "\n",
    "joined_df = get_joined()\n",
    "joined_df.to_csv('test-data/out.csv', index=False)\n",
    "joined_df"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_12484\\1918831377.py:29: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      field_names                  glossary  \\\n",
       "0  POLICY_NUMBERs             Policy Number   \n",
       "1      POL_NUMBER             Policy Number   \n",
       "2        COVERAGE                  Coverage   \n",
       "3  ANNUAL_PREMIUM  Guideline Annual Premium   \n",
       "\n",
       "                                          definition  spacy_similarity  \n",
       "0  A unique identifier assigned to a policy (e.g....          0.521119  \n",
       "1  A unique identifier assigned to a policy (e.g....          1.000000  \n",
       "2  A financial services agreement component detai...          1.000000  \n",
       "3  This is the premium that needs to be paid for ...          0.899214  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>glossary</th>\n",
       "      <th>definition</th>\n",
       "      <th>spacy_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>0.521119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>Coverage</td>\n",
       "      <td>A financial services agreement component detai...</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>Guideline Annual Premium</td>\n",
       "      <td>This is the premium that needs to be paid for ...</td>\n",
       "      <td>0.899214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
