{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:23:28.506642Z",
     "start_time": "2024-11-19T14:23:28.319457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import product\n",
    "from nltk.metrics import edit_distance"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "id": "7717da458e217594",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:23:30.690792Z",
     "start_time": "2024-11-19T14:23:30.686397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def camel_case_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # not needed right now:\n",
    "    # -----------------------------------------------------\n",
    "    # tokens = []\n",
    "    # for token in snake_tokenize(string):\n",
    "    #     for sub_token in space_tokenize(token):\n",
    "    #         tokens.extend(variable_tokenize(sub_token))\n",
    "    tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "id": "4f700e12fd4994aa",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:31:22.231871Z",
     "start_time": "2024-11-19T14:31:22.225895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replacement_tokens = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def normalize(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in replacement_tokens:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def tokenize_then_normalize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = normalize(tokens)\n",
    "    return tokens\n"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:31:23.540944Z",
     "start_time": "2024-11-19T14:31:23.534170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Make a fake data dictionary for testing\n",
    "\"\"\"\n",
    "dictionary_fields = [\n",
    "    \"POLICY_NUMBERs\",\n",
    "    \"POL_NUMBER\",\n",
    "    # \"policyNumber\",\n",
    "    # \"polNo\",\n",
    "    # \"PolicyNumber\",\n",
    "    \"COVERAGE\",\n",
    "    \"ANNUAL_PREMIUM\"\n",
    "]\n",
    "\n",
    "dictionary_df = pandas.DataFrame(\n",
    "    dictionary_fields,\n",
    "    columns=[\"field_names\"])\n",
    "\n",
    "dictionary_df['tokenized_field_names'] = (\n",
    "    dictionary_df['field_names'].apply(tokenize_then_normalize))\n",
    "\n",
    "dictionary_df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      field_names tokenized_field_names\n",
       "0  POLICY_NUMBERs     [policy, numbers]\n",
       "1      POL_NUMBER      [policy, number]\n",
       "2        COVERAGE            [coverage]\n",
       "3  ANNUAL_PREMIUM     [annual, premium]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>tokenized_field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>[policy, numbers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>[policy, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>[coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>[annual, premium]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "id": "8e97c24098aa98c1",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:31:57.966273Z",
     "start_time": "2024-11-19T14:31:57.905262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Read the glossary data\n",
    "\"\"\"\n",
    "acord_df = pandas.read_csv(\n",
    "    'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "    header=0)\n",
    "\n",
    "acord_df['tokenized_glossary'] = (\n",
    "    acord_df['Glossary Terms'].apply(tokenize_then_normalize))\n",
    "\n",
    "print(acord_df.head())"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Glossary Terms  \\\n",
      "0                   A\" rates\"   \n",
      "1             A I Or Robotics   \n",
      "2                         A&E   \n",
      "3  A-Share Variable Annuities   \n",
      "4            A.M. Best rating   \n",
      "\n",
      "                                          Definition    Status  \\\n",
      "0  Judgment rates that do not have loss experienc...  Released   \n",
      "1  Artificial intelligence is intelligence demons...  Released   \n",
      "2                     Architects and Engineers (A&E)  Released   \n",
      "3  Annuity contracts in which sales charges are i...  Released   \n",
      "4  An evaluation published by A.M. Best Company o...  Released   \n",
      "\n",
      "  Status Eff. Release Documentation Source(s) Synonym(s)  \\\n",
      "0          Glos-M 2.7                                      \n",
      "1         Info-M 2.11                                      \n",
      "2          Glos-M 2.7                                      \n",
      "3          Glos-M 2.7                                      \n",
      "4          Glos-M 2.7                                      \n",
      "\n",
      "                                           GUID             Framework Facet  \\\n",
      "0  _2022x_2_eb7034d_1718994655342_654318_132328             AF_Glossary 2.x   \n",
      "1  _2022x_1_eb7034d_1690314630859_297260_196910                   AF_IM 2.x   \n",
      "2  _2022x_1_eb7034d_1690314627649_117692_172594  Abbreviations and Acronyms   \n",
      "3  _2022x_1_eb7034d_1690314628293_767223_178085           Glossary Of Terms   \n",
      "4  _2022x_1_eb7034d_1690314624241_438614_147697           Glossary Of Terms   \n",
      "\n",
      "                                      Qualified Name   Element Type  \\\n",
      "0  AF_Glossary 2.x::ACORD Business Glossary 2.13:...  Glossary Term   \n",
      "1  AF_IM 2.x::ACORD Information Model 2.11::Party...          Class   \n",
      "2  AF_Glossary 2.x::ACORD Business Glossary 2.13:...  Glossary Term   \n",
      "3  AF_Glossary 2.x::ACORD Business Glossary 2.13:...  Glossary Term   \n",
      "4  AF_Glossary 2.x::ACORD Business Glossary 2.13:...  Glossary Term   \n",
      "\n",
      "  Initial Release Last Modified Release Documentation Modified Release  \\\n",
      "0      Glos-M 2.7                   NaN                     Glos-M 2.7   \n",
      "1     Info-M 2.11           Info-M 2.11                    Info-M 2.11   \n",
      "2      Glos-M 2.7                   NaN                     Glos-M 2.7   \n",
      "3      Glos-M 2.7                   NaN                     Glos-M 2.7   \n",
      "4      Glos-M 2.7                   NaN                     Glos-M 2.7   \n",
      "\n",
      "         Stereotypes SuperClasses Is Acronym              tokenized_glossary  \n",
      "0  acordGlossaryTerm                     NaN                      [a, rates]  \n",
      "1  acordGlossaryTerm                     NaN            [a, i, or, robotics]  \n",
      "2  acordGlossaryTerm                    True                           [a&e]  \n",
      "3  acordGlossaryTerm                     NaN  [a-share, variable, annuities]  \n",
      "4  acordGlossaryTerm                     NaN            [a.m., best, rating]  \n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "id": "f3df8d63c056072e",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:32:15.029903Z",
     "start_time": "2024-11-19T14:32:15.023587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "attempts to handle out of order words in each token list.\n",
    "\"\"\"\n",
    "def best_paired_tokens_edit_distance(tokenized_term1, tokenized_term2):\n",
    "\n",
    "    # get the best matched tokens from a list of tuples\n",
    "    #   each tuple has a \"target\" token, a \"potential match\" token, and a similarity score\n",
    "    def best_matches(tuples_list):\n",
    "\n",
    "        best = {}\n",
    "        for target, potential_match, score in tuples_list:\n",
    "            # Check if we have seen this target before or if the current score is better\n",
    "            if target not in best or score < best[target][2]:\n",
    "                best[target] = (target, potential_match, score)\n",
    "\n",
    "        return list(best.values())\n",
    "\n",
    "\n",
    "    # make unique pairs\n",
    "    l1 = list(set(tokenized_term1))\n",
    "    l2 = list(set(tokenized_term2))\n",
    "    pairs = product(l1, l2)\n",
    "\n",
    "    # calculate the Jaccard distance between all pairs\n",
    "    token_distances = [(token1, token2, edit_distance(token1, token2))\n",
    "                       for token1, token2 in pairs]\n",
    "\n",
    "\n",
    "    best = best_matches(token_distances)\n",
    "    # todo: the total distance needs to handle the \"extra\" fields in each token list that are not \"best matches\"\n",
    "    #    e.g., \"policy number\" and \"the policy number\" has an extra \"the\" in the second list of tokens.\n",
    "    #    and vice versa\n",
    "    total_distance = sum(token_distance[2] for token_distance in best)\n",
    "\n",
    "    if len(tokenized_term2) - len(tokenized_term1) > 0:\n",
    "        extra_terms = len(tokenized_term2) - len(tokenized_term1)\n",
    "        extra_term_penalty = extra_terms + (extra_terms * total_distance)\n",
    "        # extra_term_penalty = 0\n",
    "    else:\n",
    "        extra_term_penalty = 0\n",
    "\n",
    "    return total_distance + extra_term_penalty"
   ],
   "id": "f3df8d63c056072e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "id": "d9f1c3b665797f44",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:32:15.678749Z",
     "start_time": "2024-11-19T14:32:15.675012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_edit_distance(tokenized_term1, tokenized_term2):\n",
    "    total_distance = 0\n",
    "\n",
    "    if len(tokenized_term1) == len(tokenized_term2):\n",
    "        # modification on pure edit distance of the entire token list: if the lists are the same length\n",
    "        #    then discount the distance when two tokens start with the same sequence\n",
    "        #    for example: `policy` and `form` are the same distance from `pol` but `pol` is far\n",
    "        #    more likely to be closer to `policy`\n",
    "        for (token1, token2) in zip(tokenized_term1, tokenized_term2):\n",
    "            my_distance = edit_distance(token1, token2)\n",
    "            my_distance = my_distance / 2 if token2.startswith(token1) else my_distance\n",
    "            total_distance = total_distance + my_distance\n",
    "    else:\n",
    "        # otherwise join the lists back together with spaces (to preserve `token differentiation`)\n",
    "        #    and edit distance those strings\n",
    "        space = \" \"\n",
    "        string1 = space.join(tokenized_term1).strip()\n",
    "        string2 = space.join(tokenized_term2).strip()\n",
    "        total_distance = edit_distance(string1, string2)\n",
    "\n",
    "    return total_distance"
   ],
   "id": "d9f1c3b665797f44",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "id": "f12016d8f5e2661e",
    "outputId": "6e86c791-b588-4fe8-dda1-bf6d12b5305e",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:32:16.804655Z",
     "start_time": "2024-11-19T14:32:16.800895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"a\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"Policy Number\")))\n",
    "print(\"b\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"From Number\")))\n",
    "print(\"c\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Policy Number\")))\n",
    "print(\"d\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Form Number\")))"
   ],
   "id": "f12016d8f5e2661e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0\n",
      "b 6.0\n",
      "c 4\n",
      "d 8\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "id": "e086e12a4fe45428",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:32:18.701363Z",
     "start_time": "2024-11-19T14:32:18.695512Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_closest_match(target_df=None, target_field=None, match_df=None, match_field=None, match_algorithm=None, match_score_field=None, matches_df=None):\n",
    "    # Prepare an empty list to store closest matches\n",
    "    closest_matches = []\n",
    "\n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, dict_tokens in target_df[target_field].items():\n",
    "        # print('> ', idx, dict_tokens)\n",
    "        best_similarity = float('inf')\n",
    "        best_match_idx = None\n",
    "\n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, acord_tokens in match_df[match_field].items():\n",
    "            similarity = match_algorithm(dict_tokens, acord_tokens)\n",
    "            # print('> ', similarity, best_similarity, dict_tokens, acord_tokens)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "\n",
    "        # Append the best match for the current dictionary token\n",
    "        closest_matches.append([idx, best_match_idx, best_similarity])\n",
    "\n",
    "    return pandas.DataFrame(closest_matches, columns=[\n",
    "        \"target_index\",\n",
    "        match_score_field + \"_match_index\",\n",
    "        match_score_field\n",
    "    ])"
   ],
   "id": "e086e12a4fe45428",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:34:11.769923Z",
     "start_time": "2024-11-19T14:34:08.173973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_algorithms = [\n",
    "    (\"modified_edit_distance\", modified_edit_distance),\n",
    "    (\"best_paired_tokens_edit_distance\", best_paired_tokens_edit_distance),\n",
    "    # (\"similarity3\", modified_edit_distance),\n",
    "]\n",
    "\n",
    "matches_df = None\n",
    "for match_score_field, match_algorithm in match_algorithms:\n",
    "    # Call the `find_closest_match` function\n",
    "    my_matches_df = find_closest_match(\n",
    "        target_df=dictionary_df,\n",
    "        target_field='tokenized_field_names',\n",
    "        match_df=acord_df,\n",
    "        match_field='tokenized_glossary',\n",
    "        match_algorithm=match_algorithm,\n",
    "        match_score_field=match_score_field\n",
    "    )\n",
    "\n",
    "    # If final_df is None, set it to matches_df\n",
    "    if matches_df is None:\n",
    "        matches_df = my_matches_df\n",
    "    else:\n",
    "        matches_df = matches_df.merge(my_matches_df, on='target_index')\n",
    "\n",
    "\n",
    "# best_score_field, best_algo = max(match_algorithms, key=lambda x: matches_df[x[0]].max())\n",
    "\n",
    "match_metadata = [\n",
    "    (score_field, score_field + '_match_index', algo.__name__)\n",
    "    for score_field, algo in match_algorithms\n",
    "]\n",
    "\n",
    "print(match_metadata)\n",
    "matches_df['closest_similar_index'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['best_paired_tokens_edit_distance_match_index'],\n",
    "    matches_df['modified_edit_distance_match_index']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_score'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['modified_edit_distance']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_algorithm'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    'best_paired_tokens_edit_distance',\n",
    "    'modified_edit_distance'\n",
    ")\n",
    "# print (source_token_within_search_edit_distance.__name__)\n",
    "\n",
    "# matches_df"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('modified_edit_distance', 'modified_edit_distance_match_index', 'modified_edit_distance'), ('best_paired_tokens_edit_distance', 'best_paired_tokens_edit_distance_match_index', 'best_paired_tokens_edit_distance')]\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c",
    "ExecuteTime": {
     "end_time": "2024-11-19T14:34:14.734363Z",
     "start_time": "2024-11-19T14:34:14.725491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract index from target (dictionary) and reset index\n",
    "reset_dictionary_df = (dictionary_df.\n",
    "                       loc[matches_df['target_index']].\n",
    "                       reset_index(drop=True))\n",
    "\n",
    "# extract index from proposed match (acord) and reset index\n",
    "reset_acord_df = (acord_df.\n",
    "                  loc[matches_df['closest_similar_index']].\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "# join target df with matches df\n",
    "joined_df = reset_dictionary_df.join(reset_acord_df)\n",
    "\n",
    "# append similarity score\n",
    "joined_df['closest_similarity_score'] = matches_df['closest_similarity_score']\n",
    "joined_df['closest_similarity_algorithm'] = matches_df['closest_similarity_algorithm']\n",
    "\n",
    "joined_df = joined_df[[\n",
    "    'field_names',\n",
    "    'Glossary Terms',\n",
    "    # 'Definition',\n",
    "    'closest_similarity_score',\n",
    "    'closest_similarity_algorithm'\n",
    "]]\n",
    "\n",
    "print(joined_df.head(10))\n",
    "joined_df.to_csv('test-data/out.csv', index=False)"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      field_names            Glossary Terms  closest_similarity_score  \\\n",
      "0  POLICY_NUMBERs             Policy Number                       1.0   \n",
      "1      POL_NUMBER             Policy Number                       0.0   \n",
      "2        COVERAGE                  Coverage                       0.0   \n",
      "3  ANNUAL_PREMIUM  Guideline Annual Premium                       1.0   \n",
      "\n",
      "       closest_similarity_algorithm  \n",
      "0            modified_edit_distance  \n",
      "1            modified_edit_distance  \n",
      "2            modified_edit_distance  \n",
      "3  best_paired_tokens_edit_distance  \n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b50cc05637aa6576"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
