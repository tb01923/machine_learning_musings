{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T00:44:12.103646Z",
     "start_time": "2024-11-23T00:44:12.100296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# spacy.info()\n",
    "\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# !C:\\\\Users\\\\tbadmin\\\\Documents\\\\projects\\\\machine_learning_musings\\\\.venv\n"
   ],
   "id": "687a6eef1880ff24",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:38:39.901720Z",
     "start_time": "2024-11-23T14:38:37.666210Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "import spacy\n",
    "from itertools import product\n",
    "from nltk.metrics import edit_distance\n",
    "# from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup #pip install beautifulsoup4\n",
    "\n",
    "# hack because venv nonsense\n",
    "# model = 'en_core_web_md'\n",
    "# model = 'en_core_web_sm'\n",
    "model = 'en_core_web_lg'\n",
    "model_location = \"..\\\\.venv\\\\Lib\\\\site-packages\\\\\" + model + \"\\\\\" + model + \"-3.8.0\"\n",
    "\n",
    "nlp = spacy.load(model_location) # python -m spacy download en_core_web_lg\n",
    "# spacy.info()\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "id": "7717da458e217594",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:38:40.936980Z",
     "start_time": "2024-11-23T14:38:40.931261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def camel_case_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # not needed right now:\n",
    "    # -----------------------------------------------------\n",
    "    # tokens = []\n",
    "    # for token in snake_tokenize(string):\n",
    "    #     for sub_token in space_tokenize(token):\n",
    "    #         tokens.extend(variable_tokenize(sub_token))\n",
    "    tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "id": "4f700e12fd4994aa",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:38:41.528229Z",
     "start_time": "2024-11-23T14:38:41.524175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replacement_tokens = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def normalize(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in replacement_tokens:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def tokenize_then_normalize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = normalize(tokens)\n",
    "    return tokens\n"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:38:42.183151Z",
     "start_time": "2024-11-23T14:38:42.155395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_nlp(tokens):\n",
    "    return nlp(\" \".join(tokens))\n",
    "\n",
    "def prepare_df(df, field, result_prefix=None):\n",
    "    if result_prefix is None:\n",
    "        df['tokenized_' + field] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "        df['nlp_' + field] = (\n",
    "            df['tokenized_' + field].apply(to_nlp))\n",
    "    else:\n",
    "        df[result_prefix] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "        df['nlp_' + result_prefix] = (\n",
    "            df[result_prefix].apply(to_nlp))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Make a fake data dictionary for testing\n",
    "\"\"\"\n",
    "dictionary_fields = [\n",
    "    \"POLICY_NUMBERs\",\n",
    "    \"POL_NUMBER\",\n",
    "    # \"policyNumber\",\n",
    "    # \"polNo\",\n",
    "    # \"PolicyNumber\",\n",
    "    \"COVERAGE\",\n",
    "    \"ANNUAL_PREMIUM\"\n",
    "]\n",
    "\n",
    "dictionary_df = pandas.DataFrame(\n",
    "    dictionary_fields,\n",
    "    columns=[\"field_names\"])\n",
    "\n",
    "dictionary_df = prepare_df(dictionary_df, \"field_names\")\n",
    "dictionary_df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      field_names tokenized_field_names    nlp_field_names\n",
       "0  POLICY_NUMBERs     [policy, numbers]  (policy, numbers)\n",
       "1      POL_NUMBER      [policy, number]   (policy, number)\n",
       "2        COVERAGE            [coverage]         (coverage)\n",
       "3  ANNUAL_PREMIUM     [annual, premium]  (annual, premium)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>tokenized_field_names</th>\n",
       "      <th>nlp_field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>[policy, numbers]</td>\n",
       "      <td>(policy, numbers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>[policy, number]</td>\n",
       "      <td>(policy, number)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>[coverage]</td>\n",
       "      <td>(coverage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>[annual, premium]</td>\n",
       "      <td>(annual, premium)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "id": "8e97c24098aa98c1",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:10.561573Z",
     "start_time": "2024-11-23T14:38:50.895977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Read the glossary data\n",
    "\"\"\"\n",
    "acord_df = pandas.read_csv(\n",
    "    'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "    header=0)\n",
    "\n",
    "acord_df = prepare_df(acord_df, \"Glossary Terms\", 'tokenized_glossary') \n",
    "\n",
    "acord_df[['Glossary Terms', 'tokenized_glossary', 'nlp_tokenized_glossary']]"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                Glossary Terms  \\\n",
       "0                                    A\" rates\"   \n",
       "1                              A I Or Robotics   \n",
       "2                                          A&E   \n",
       "3                   A-Share Variable Annuities   \n",
       "4                             A.M. Best rating   \n",
       "...                                        ...   \n",
       "6227                                      eEg7   \n",
       "6228  excess and surplus (E&S) lines insurance   \n",
       "6229                           fringe benefits   \n",
       "6230                                      hSOD   \n",
       "6231      tobacco Cessation Program Indicator    \n",
       "\n",
       "                                   tokenized_glossary  \\\n",
       "0                                          [a, rates]   \n",
       "1                                [a, i, or, robotics]   \n",
       "2                                               [a&e]   \n",
       "3                      [a-share, variable, annuities]   \n",
       "4                                [a.m., best, rating]   \n",
       "...                                               ...   \n",
       "6227                                           [eeg7]   \n",
       "6228  [excess, and, surplus, (e&s), lines, insurance]   \n",
       "6229                               [fringe, benefits]   \n",
       "6230                                           [hsod]   \n",
       "6231       [tobacco, cessation, program, indicator, ]   \n",
       "\n",
       "                                 nlp_tokenized_glossary  \n",
       "0                                            (a, rates)  \n",
       "1                                  (a, i, or, robotics)  \n",
       "2                                                 (a&e)  \n",
       "3                    (a, -, share, variable, annuities)  \n",
       "4                                  (a.m., best, rating)  \n",
       "...                                                 ...  \n",
       "6227                                             (eeg7)  \n",
       "6228  (excess, and, surplus, (, e&s, ), lines, insur...  \n",
       "6229                                 (fringe, benefits)  \n",
       "6230                                             (hsod)  \n",
       "6231           (tobacco, cessation, program, indicator)  \n",
       "\n",
       "[6232 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glossary Terms</th>\n",
       "      <th>tokenized_glossary</th>\n",
       "      <th>nlp_tokenized_glossary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A\" rates\"</td>\n",
       "      <td>[a, rates]</td>\n",
       "      <td>(a, rates)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A I Or Robotics</td>\n",
       "      <td>[a, i, or, robotics]</td>\n",
       "      <td>(a, i, or, robotics)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A&amp;E</td>\n",
       "      <td>[a&amp;e]</td>\n",
       "      <td>(a&amp;e)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-Share Variable Annuities</td>\n",
       "      <td>[a-share, variable, annuities]</td>\n",
       "      <td>(a, -, share, variable, annuities)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.M. Best rating</td>\n",
       "      <td>[a.m., best, rating]</td>\n",
       "      <td>(a.m., best, rating)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>eEg7</td>\n",
       "      <td>[eeg7]</td>\n",
       "      <td>(eeg7)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>excess and surplus (E&amp;S) lines insurance</td>\n",
       "      <td>[excess, and, surplus, (e&amp;s), lines, insurance]</td>\n",
       "      <td>(excess, and, surplus, (, e&amp;s, ), lines, insur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>fringe benefits</td>\n",
       "      <td>[fringe, benefits]</td>\n",
       "      <td>(fringe, benefits)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6230</th>\n",
       "      <td>hSOD</td>\n",
       "      <td>[hsod]</td>\n",
       "      <td>(hsod)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6231</th>\n",
       "      <td>tobacco Cessation Program Indicator</td>\n",
       "      <td>[tobacco, cessation, program, indicator, ]</td>\n",
       "      <td>(tobacco, cessation, program, indicator)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6232 rows × 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "id": "f3df8d63c056072e",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:23.567586Z",
     "start_time": "2024-11-23T14:39:23.560708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "attempts to handle out of order words in each token list.\n",
    "\"\"\"\n",
    "def best_paired_tokens_edit_distance(tokenized_term1, tokenized_term2):\n",
    "\n",
    "    # get the best matched tokens from a list of tuples\n",
    "    #   each tuple has a \"target\" token, a \"potential match\" token, and a similarity score\n",
    "    def best_matches(tuples_list):\n",
    "\n",
    "        best = {}\n",
    "        for target, potential_match, score in tuples_list:\n",
    "            # Check if we have seen this target before or if the current score is better\n",
    "            if target not in best or score < best[target][2]:\n",
    "                best[target] = (target, potential_match, score)\n",
    "\n",
    "        return list(best.values())\n",
    "\n",
    "\n",
    "    # make unique pairs\n",
    "    l1 = list(set(tokenized_term1))\n",
    "    l2 = list(set(tokenized_term2))\n",
    "    pairs = product(l1, l2)\n",
    "\n",
    "    # calculate the Jaccard distance between all pairs\n",
    "    token_distances = [(token1, token2, edit_distance(token1, token2))\n",
    "                       for token1, token2 in pairs]\n",
    "\n",
    "\n",
    "    best = best_matches(token_distances)\n",
    "    # todo: the total distance needs to handle the \"extra\" fields in each token list that are not \"best matches\"\n",
    "    #    e.g., \"policy number\" and \"the policy number\" has an extra \"the\" in the second list of tokens.\n",
    "    #    and vice versa\n",
    "    total_distance = sum(token_distance[2] for token_distance in best)\n",
    "\n",
    "    if len(tokenized_term2) - len(tokenized_term1) > 0:\n",
    "        extra_terms = len(tokenized_term2) - len(tokenized_term1)\n",
    "        extra_term_penalty = extra_terms + (extra_terms * total_distance)\n",
    "        # extra_term_penalty = 0\n",
    "    else:\n",
    "        extra_term_penalty = 0\n",
    "\n",
    "    return total_distance + extra_term_penalty"
   ],
   "id": "f3df8d63c056072e",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "id": "d9f1c3b665797f44",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:24.287530Z",
     "start_time": "2024-11-23T14:39:24.281218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_edit_distance(tokenized_term1, tokenized_term2):\n",
    "    total_distance = 0\n",
    "\n",
    "    if len(tokenized_term1) == len(tokenized_term2):\n",
    "        # modification on pure edit distance of the entire token list: if the lists are the same length\n",
    "        #    then discount the distance when two tokens start with the same sequence\n",
    "        #    for example: `policy` and `form` are the same distance from `pol` but `pol` is far\n",
    "        #    more likely to be closer to `policy`\n",
    "        for (token1, token2) in zip(tokenized_term1, tokenized_term2):\n",
    "            my_distance = edit_distance(token1, token2)\n",
    "            my_distance = my_distance / 2 if token2.startswith(token1) else my_distance\n",
    "            total_distance = total_distance + my_distance\n",
    "    else:\n",
    "        # otherwise join the lists back together with spaces (to preserve `token differentiation`)\n",
    "        #    and edit distance those strings\n",
    "        space = \" \"\n",
    "        string1 = space.join(tokenized_term1).strip()\n",
    "        string2 = space.join(tokenized_term2).strip()\n",
    "        total_distance = edit_distance(string1, string2)\n",
    "\n",
    "    return total_distance"
   ],
   "id": "d9f1c3b665797f44",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:24.673401Z",
     "start_time": "2024-11-23T14:39:24.667836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def spacy_similarity(doc1, doc2):\n",
    "    # spacy returns 1 for a perfect match and 0 for no match\n",
    "    # spacy scaling is 0..1\n",
    "    # therefore reverse and multiply by 10 to make it look similar to edit distance\n",
    "    # if this is the right algo, we would just change the rest of the  code as opposed to the adj\n",
    "    similarity = (1 - doc1.similarity(doc2)) * 10\n",
    "    return similarity"
   ],
   "id": "6669cd7a405a3857",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "id": "f12016d8f5e2661e",
    "outputId": "6e86c791-b588-4fe8-dda1-bf6d12b5305e",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:25.090169Z",
     "start_time": "2024-11-23T14:39:25.085897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"a\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"Policy Number\")))\n",
    "print(\"b\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"From Number\")))\n",
    "print(\"c\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Policy Number\")))\n",
    "print(\"d\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Form Number\")))"
   ],
   "id": "f12016d8f5e2661e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0\n",
      "b 6.0\n",
      "c 4\n",
      "d 8\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "id": "e086e12a4fe45428",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:25.930714Z",
     "start_time": "2024-11-23T14:39:25.924839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_closest_match(target_df=None, target_field=None, match_df=None, match_field=None, match_algorithm=None, match_score_field=None, matches_df=None):\n",
    "    # Prepare an empty list to store closest matches\n",
    "    closest_matches = []\n",
    "\n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, dict_tokens in target_df[target_field].items():\n",
    "        # print('> ', idx, dict_tokens)\n",
    "        best_similarity = float('inf')\n",
    "        best_match_idx = None\n",
    "\n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, acord_tokens in match_df[match_field].items():\n",
    "            similarity = match_algorithm(dict_tokens, acord_tokens)\n",
    "            # print('> ', similarity, best_similarity, dict_tokens, acord_tokens)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "\n",
    "        # Append the best match for the current dictionary token\n",
    "        closest_matches.append([idx, best_match_idx, best_similarity])\n",
    "\n",
    "    return pandas.DataFrame(closest_matches, columns=[\n",
    "        \"target_index\",\n",
    "        match_score_field + \"_match_index\",\n",
    "        match_score_field\n",
    "    ])"
   ],
   "id": "e086e12a4fe45428",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:27.665873Z",
     "start_time": "2024-11-23T14:39:27.377394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_algorithms = [\n",
    "    # (\"modified_edit_distance\", modified_edit_distance),\n",
    "    # (\"best_paired_tokens_edit_distance\", best_paired_tokens_edit_distance),\n",
    "    (\"spacy_similarity\", spacy_similarity),\n",
    "]\n",
    "\n",
    "matches_df = None\n",
    "for match_score_field, match_algorithm in match_algorithms:\n",
    "    # Call the `find_closest_match` function\n",
    "    my_matches_df = find_closest_match(\n",
    "        target_df=dictionary_df,\n",
    "        # target_field='tokenized_field_names',\n",
    "        target_field='nlp_field_names',\n",
    "        match_df=acord_df,\n",
    "        # match_field='tokenized_glossary',\n",
    "        match_field='nlp_tokenized_glossary',\n",
    "        match_algorithm=match_algorithm,\n",
    "        match_score_field=match_score_field\n",
    "    )\n",
    "\n",
    "    # If final_df is None, set it to matches_df\n",
    "    if matches_df is None:\n",
    "        matches_df = my_matches_df\n",
    "    else:\n",
    "        matches_df = matches_df.merge(my_matches_df, on='target_index')\n",
    "\n",
    "matches_df\n",
    "# best_score_field, best_algo = max(match_algorithms, key=lambda x: matches_df[x[0]].max())"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_10860\\3976292537.py:6: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  similarity = (1 - doc1.similarity(doc2)) * 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   target_index  spacy_similarity_match_index  spacy_similarity\n",
       "0             0                          4811          0.644723\n",
       "1             1                          4811          0.000000\n",
       "2             2                          1649          0.000000\n",
       "3             3                          3021          1.007857"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_index</th>\n",
       "      <th>spacy_similarity_match_index</th>\n",
       "      <th>spacy_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.644723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.007857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:29.302483Z",
     "start_time": "2024-11-23T14:39:29.289134Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_metadata = [\n",
    "    (score_field, score_field + '_match_index', algo.__name__)\n",
    "    for score_field, algo in match_algorithms\n",
    "]\n",
    "\n",
    "matches_df['closest_similar_index'] = matches_df[\"spacy_similarity_match_index\"]\n",
    "matches_df['closest_similarity_score'] = matches_df[\"spacy_similarity\"]\n",
    "matches_df['closest_similarity_algorithm'] = \"spacy_similarity\" \n",
    "\n",
    "## using the edit distance logic\n",
    "\n",
    "# print(match_metadata)\n",
    "# matches_df['closest_similar_index'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['best_paired_tokens_edit_distance_match_index'],\n",
    "#     matches_df['modified_edit_distance_match_index']\n",
    "# )\n",
    "# \n",
    "# matches_df['closest_similarity_score'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['modified_edit_distance']\n",
    "# )\n",
    "# \n",
    "# matches_df['closest_similarity_algorithm'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     'best_paired_tokens_edit_distance',\n",
    "#     'modified_edit_distance'\n",
    "# )\n",
    "# print (source_token_within_search_edit_distance.__name__)\n",
    "\n",
    "matches_df"
   ],
   "id": "da03474048283783",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   target_index  spacy_similarity_match_index  spacy_similarity  \\\n",
       "0             0                          4811          0.644723   \n",
       "1             1                          4811          0.000000   \n",
       "2             2                          1649          0.000000   \n",
       "3             3                          3021          1.007857   \n",
       "\n",
       "   closest_similar_index  closest_similarity_score  \\\n",
       "0                   4811                  0.644723   \n",
       "1                   4811                  0.000000   \n",
       "2                   1649                  0.000000   \n",
       "3                   3021                  1.007857   \n",
       "\n",
       "  closest_similarity_algorithm  \n",
       "0             spacy_similarity  \n",
       "1             spacy_similarity  \n",
       "2             spacy_similarity  \n",
       "3             spacy_similarity  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_index</th>\n",
       "      <th>spacy_similarity_match_index</th>\n",
       "      <th>spacy_similarity</th>\n",
       "      <th>closest_similar_index</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.644723</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.644723</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.007857</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.007857</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c",
    "ExecuteTime": {
     "end_time": "2024-11-23T14:39:30.581515Z",
     "start_time": "2024-11-23T14:39:30.566237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract index from target (dictionary) and reset index\n",
    "reset_dictionary_df = (dictionary_df.\n",
    "                       loc[matches_df['target_index']].\n",
    "                       reset_index(drop=True))\n",
    "\n",
    "# extract index from proposed match (acord) and reset index\n",
    "reset_acord_df = (acord_df.\n",
    "                  loc[matches_df['closest_similar_index']].\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "# join target df with matches df\n",
    "joined_df = reset_dictionary_df.join(reset_acord_df)\n",
    "\n",
    "# append similarity score\n",
    "def stripHtml (html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "    \n",
    "joined_df['closest_similarity_score'] = matches_df['closest_similarity_score']\n",
    "joined_df['closest_similarity_algorithm'] = matches_df['closest_similarity_algorithm']\n",
    "\n",
    "joined_df['Definition'] =  (\n",
    "    joined_df['Definition'].apply(stripHtml))\n",
    "\n",
    "joined_df = joined_df[[\n",
    "    'field_names',\n",
    "    'Glossary Terms',\n",
    "    'Definition',\n",
    "    'closest_similarity_score',\n",
    "    'closest_similarity_algorithm'\n",
    "]]\n",
    "\n",
    "joined_df.to_csv('test-data/out.csv', index=False)\n",
    "joined_df"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_10860\\3526957015.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      field_names            Glossary Terms  \\\n",
       "0  POLICY_NUMBERs             Policy Number   \n",
       "1      POL_NUMBER             Policy Number   \n",
       "2        COVERAGE                  Coverage   \n",
       "3  ANNUAL_PREMIUM  Guideline Annual Premium   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  A unique identifier assigned to a policy (e.g....   \n",
       "1  A unique identifier assigned to a policy (e.g....   \n",
       "2  A financial services agreement component detai...   \n",
       "3  This is the premium that needs to be paid for ...   \n",
       "\n",
       "   closest_similarity_score closest_similarity_algorithm  \n",
       "0                  0.644723             spacy_similarity  \n",
       "1                  0.000000             spacy_similarity  \n",
       "2                  0.000000             spacy_similarity  \n",
       "3                  1.007857             spacy_similarity  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>Glossary Terms</th>\n",
       "      <th>Definition</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>0.644723</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>Coverage</td>\n",
       "      <td>A financial services agreement component detai...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>Guideline Annual Premium</td>\n",
       "      <td>This is the premium that needs to be paid for ...</td>\n",
       "      <td>1.007857</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b50cc05637aa6576",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
