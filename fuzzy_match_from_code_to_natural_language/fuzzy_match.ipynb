{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T13:58:13.560498Z",
     "start_time": "2024-11-22T13:58:13.556587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# spacy.info()\n",
    "\n",
    "# import os\n",
    "# os.getcwd()\n",
    "# !C:\\\\Users\\\\tbadmin\\\\Documents\\\\projects\\\\machine_learning_musings\\\\.venv\n"
   ],
   "id": "687a6eef1880ff24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\tbadmin\\\\Documents\\\\projects\\\\machine_learning_musings\\\\fuzzy_match_from_code_to_natural_language'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:16:20.451870Z",
     "start_time": "2024-11-22T14:16:16.134773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "import spacy\n",
    "from itertools import product\n",
    "from nltk.metrics import edit_distance\n",
    "# from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup #pip install beautifulsoup4\n",
    "\n",
    "# hack because venv nonsense\n",
    "model = 'en_core_web_lg'\n",
    "model_location = \"..\\\\.venv\\\\Lib\\\\site-packages\\\\\" + model + \"\\\\\" + model + \"-3.8.0\"\n",
    "\n",
    "nlp = spacy.load(model_location) # python -m spacy download en_core_web_lg\n",
    "# spacy.info()\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "id": "7717da458e217594",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:16:31.711151Z",
     "start_time": "2024-11-22T14:16:31.700395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def camel_case_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # not needed right now:\n",
    "    # -----------------------------------------------------\n",
    "    # tokens = []\n",
    "    # for token in snake_tokenize(string):\n",
    "    #     for sub_token in space_tokenize(token):\n",
    "    #         tokens.extend(variable_tokenize(sub_token))\n",
    "    tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "id": "4f700e12fd4994aa",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:16:32.689808Z",
     "start_time": "2024-11-22T14:16:32.676186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replacement_tokens = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def normalize(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in replacement_tokens:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def tokenize_then_normalize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = normalize(tokens)\n",
    "    return tokens\n"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:16:34.415136Z",
     "start_time": "2024-11-22T14:16:34.341775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_nlp(tokens):\n",
    "    return nlp(\" \".join(tokens))\n",
    "\n",
    "def prepare_df(df, field, result_prefix=None):\n",
    "    if result_prefix is None:\n",
    "        df['tokenized_' + field] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "        df['nlp_' + field] = (\n",
    "            df['tokenized_' + field].apply(to_nlp))\n",
    "    else:\n",
    "        df[result_prefix] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "        df['nlp_' + result_prefix] = (\n",
    "            df[result_prefix].apply(to_nlp))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Make a fake data dictionary for testing\n",
    "\"\"\"\n",
    "dictionary_fields = [\n",
    "    \"POLICY_NUMBERs\",\n",
    "    \"POL_NUMBER\",\n",
    "    # \"policyNumber\",\n",
    "    # \"polNo\",\n",
    "    # \"PolicyNumber\",\n",
    "    \"COVERAGE\",\n",
    "    \"ANNUAL_PREMIUM\"\n",
    "]\n",
    "\n",
    "dictionary_df = pandas.DataFrame(\n",
    "    dictionary_fields,\n",
    "    columns=[\"field_names\"])\n",
    "\n",
    "dictionary_df = prepare_df(dictionary_df, \"field_names\")\n",
    "dictionary_df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      field_names tokenized_field_names    nlp_field_names\n",
       "0  POLICY_NUMBERs     [policy, numbers]  (policy, numbers)\n",
       "1      POL_NUMBER      [policy, number]   (policy, number)\n",
       "2        COVERAGE            [coverage]         (coverage)\n",
       "3  ANNUAL_PREMIUM     [annual, premium]  (annual, premium)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>tokenized_field_names</th>\n",
       "      <th>nlp_field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>[policy, numbers]</td>\n",
       "      <td>(policy, numbers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>[policy, number]</td>\n",
       "      <td>(policy, number)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>[coverage]</td>\n",
       "      <td>(coverage)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>[annual, premium]</td>\n",
       "      <td>(annual, premium)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "id": "8e97c24098aa98c1",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-22T14:16:38.368327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Read the glossary data\n",
    "\"\"\"\n",
    "acord_df = pandas.read_csv(\n",
    "    'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "    header=0)\n",
    "\n",
    "acord_df = prepare_df(acord_df, \"Glossary Terms\", 'tokenized_glossary') \n",
    "\n",
    "acord_df[['Glossary Terms', 'tokenized_glossary', 'nlp_tokenized_glossary']]"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f3df8d63c056072e",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:05:50.938895Z",
     "start_time": "2024-11-22T14:05:50.921616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "attempts to handle out of order words in each token list.\n",
    "\"\"\"\n",
    "def best_paired_tokens_edit_distance(tokenized_term1, tokenized_term2):\n",
    "\n",
    "    # get the best matched tokens from a list of tuples\n",
    "    #   each tuple has a \"target\" token, a \"potential match\" token, and a similarity score\n",
    "    def best_matches(tuples_list):\n",
    "\n",
    "        best = {}\n",
    "        for target, potential_match, score in tuples_list:\n",
    "            # Check if we have seen this target before or if the current score is better\n",
    "            if target not in best or score < best[target][2]:\n",
    "                best[target] = (target, potential_match, score)\n",
    "\n",
    "        return list(best.values())\n",
    "\n",
    "\n",
    "    # make unique pairs\n",
    "    l1 = list(set(tokenized_term1))\n",
    "    l2 = list(set(tokenized_term2))\n",
    "    pairs = product(l1, l2)\n",
    "\n",
    "    # calculate the Jaccard distance between all pairs\n",
    "    token_distances = [(token1, token2, edit_distance(token1, token2))\n",
    "                       for token1, token2 in pairs]\n",
    "\n",
    "\n",
    "    best = best_matches(token_distances)\n",
    "    # todo: the total distance needs to handle the \"extra\" fields in each token list that are not \"best matches\"\n",
    "    #    e.g., \"policy number\" and \"the policy number\" has an extra \"the\" in the second list of tokens.\n",
    "    #    and vice versa\n",
    "    total_distance = sum(token_distance[2] for token_distance in best)\n",
    "\n",
    "    if len(tokenized_term2) - len(tokenized_term1) > 0:\n",
    "        extra_terms = len(tokenized_term2) - len(tokenized_term1)\n",
    "        extra_term_penalty = extra_terms + (extra_terms * total_distance)\n",
    "        # extra_term_penalty = 0\n",
    "    else:\n",
    "        extra_term_penalty = 0\n",
    "\n",
    "    return total_distance + extra_term_penalty"
   ],
   "id": "f3df8d63c056072e",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "id": "d9f1c3b665797f44",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:05:52.792716Z",
     "start_time": "2024-11-22T14:05:52.779992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_edit_distance(tokenized_term1, tokenized_term2):\n",
    "    total_distance = 0\n",
    "\n",
    "    if len(tokenized_term1) == len(tokenized_term2):\n",
    "        # modification on pure edit distance of the entire token list: if the lists are the same length\n",
    "        #    then discount the distance when two tokens start with the same sequence\n",
    "        #    for example: `policy` and `form` are the same distance from `pol` but `pol` is far\n",
    "        #    more likely to be closer to `policy`\n",
    "        for (token1, token2) in zip(tokenized_term1, tokenized_term2):\n",
    "            my_distance = edit_distance(token1, token2)\n",
    "            my_distance = my_distance / 2 if token2.startswith(token1) else my_distance\n",
    "            total_distance = total_distance + my_distance\n",
    "    else:\n",
    "        # otherwise join the lists back together with spaces (to preserve `token differentiation`)\n",
    "        #    and edit distance those strings\n",
    "        space = \" \"\n",
    "        string1 = space.join(tokenized_term1).strip()\n",
    "        string2 = space.join(tokenized_term2).strip()\n",
    "        total_distance = edit_distance(string1, string2)\n",
    "\n",
    "    return total_distance"
   ],
   "id": "d9f1c3b665797f44",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:05:53.645278Z",
     "start_time": "2024-11-22T14:05:53.636014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def spacy_similarity(doc1, doc2):\n",
    "    # spacy returns 1 for a perfect match and 0 for no match\n",
    "    # spacy scaling is 0..1\n",
    "    # therefore reverse and multiply by 10 to make it look similar to edit distance\n",
    "    # if this is the right algo, we would just change the rest of the  code as opposed to the adj\n",
    "    similarity = (1 - doc1.similarity(doc2)) * 10\n",
    "    return similarity"
   ],
   "id": "6669cd7a405a3857",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "id": "f12016d8f5e2661e",
    "outputId": "6e86c791-b588-4fe8-dda1-bf6d12b5305e",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:05:54.533125Z",
     "start_time": "2024-11-22T14:05:54.520549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"a\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"Policy Number\")))\n",
    "print(\"b\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"From Number\")))\n",
    "print(\"c\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Policy Number\")))\n",
    "print(\"d\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Form Number\")))"
   ],
   "id": "f12016d8f5e2661e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0\n",
      "b 6.0\n",
      "c 4\n",
      "d 8\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "id": "e086e12a4fe45428",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:05:56.337955Z",
     "start_time": "2024-11-22T14:05:56.327660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_closest_match(target_df=None, target_field=None, match_df=None, match_field=None, match_algorithm=None, match_score_field=None, matches_df=None):\n",
    "    # Prepare an empty list to store closest matches\n",
    "    closest_matches = []\n",
    "\n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, dict_tokens in target_df[target_field].items():\n",
    "        # print('> ', idx, dict_tokens)\n",
    "        best_similarity = float('inf')\n",
    "        best_match_idx = None\n",
    "\n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, acord_tokens in match_df[match_field].items():\n",
    "            similarity = match_algorithm(dict_tokens, acord_tokens)\n",
    "            # print('> ', similarity, best_similarity, dict_tokens, acord_tokens)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "\n",
    "        # Append the best match for the current dictionary token\n",
    "        closest_matches.append([idx, best_match_idx, best_similarity])\n",
    "\n",
    "    return pandas.DataFrame(closest_matches, columns=[\n",
    "        \"target_index\",\n",
    "        match_score_field + \"_match_index\",\n",
    "        match_score_field\n",
    "    ])"
   ],
   "id": "e086e12a4fe45428",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:11:40.841075Z",
     "start_time": "2024-11-22T14:11:40.440019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_algorithms = [\n",
    "    # (\"modified_edit_distance\", modified_edit_distance),\n",
    "    # (\"best_paired_tokens_edit_distance\", best_paired_tokens_edit_distance),\n",
    "    (\"spacy_similarity\", spacy_similarity),\n",
    "]\n",
    "\n",
    "matches_df = None\n",
    "for match_score_field, match_algorithm in match_algorithms:\n",
    "    # Call the `find_closest_match` function\n",
    "    my_matches_df = find_closest_match(\n",
    "        target_df=dictionary_df,\n",
    "        # target_field='tokenized_field_names',\n",
    "        target_field='nlp_field_names',\n",
    "        match_df=acord_df,\n",
    "        # match_field='tokenized_glossary',\n",
    "        match_field='nlp_tokenized_glossary',\n",
    "        match_algorithm=match_algorithm,\n",
    "        match_score_field=match_score_field\n",
    "    )\n",
    "\n",
    "    # If final_df is None, set it to matches_df\n",
    "    if matches_df is None:\n",
    "        matches_df = my_matches_df\n",
    "    else:\n",
    "        matches_df = matches_df.merge(my_matches_df, on='target_index')\n",
    "\n",
    "matches_df\n",
    "# best_score_field, best_algo = max(match_algorithms, key=lambda x: matches_df[x[0]].max())"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_18268\\3976292537.py:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  similarity = (1 - doc1.similarity(doc2)) * 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   target_index  spacy_similarity_match_index  spacy_similarity\n",
       "0             0                          5041          1.413947\n",
       "1             1                          4811          0.000000\n",
       "2             2                          1649          0.000000\n",
       "3             3                          3021          1.481956"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_index</th>\n",
       "      <th>spacy_similarity_match_index</th>\n",
       "      <th>spacy_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5041</td>\n",
       "      <td>1.413947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.481956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T14:13:48.066630Z",
     "start_time": "2024-11-22T14:13:48.041708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_metadata = [\n",
    "    (score_field, score_field + '_match_index', algo.__name__)\n",
    "    for score_field, algo in match_algorithms\n",
    "]\n",
    "\n",
    "matches_df['closest_similar_index'] = matches_df[\"spacy_similarity_match_index\"]\n",
    "matches_df['closest_similarity_score'] = matches_df[\"spacy_similarity\"]\n",
    "matches_df['closest_similarity_algorithm'] = \"spacy_similarity\" \n",
    "\n",
    "## using the edit distance logic\n",
    "\n",
    "# print(match_metadata)\n",
    "# matches_df['closest_similar_index'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['best_paired_tokens_edit_distance_match_index'],\n",
    "#     matches_df['modified_edit_distance_match_index']\n",
    "# )\n",
    "# \n",
    "# matches_df['closest_similarity_score'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['best_paired_tokens_edit_distance'],\n",
    "#     matches_df['modified_edit_distance']\n",
    "# )\n",
    "# \n",
    "# matches_df['closest_similarity_algorithm'] = np.where(\n",
    "#     matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "#     'best_paired_tokens_edit_distance',\n",
    "#     'modified_edit_distance'\n",
    "# )\n",
    "# print (source_token_within_search_edit_distance.__name__)\n",
    "\n",
    "matches_df"
   ],
   "id": "da03474048283783",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   target_index  spacy_similarity_match_index  spacy_similarity  \\\n",
       "0             0                          5041          1.413947   \n",
       "1             1                          4811          0.000000   \n",
       "2             2                          1649          0.000000   \n",
       "3             3                          3021          1.481956   \n",
       "\n",
       "   closest_similar_index  closest_similarity_score  \\\n",
       "0                   5041                  1.413947   \n",
       "1                   4811                  0.000000   \n",
       "2                   1649                  0.000000   \n",
       "3                   3021                  1.481956   \n",
       "\n",
       "  closest_similarity_algorithm  \n",
       "0             spacy_similarity  \n",
       "1             spacy_similarity  \n",
       "2             spacy_similarity  \n",
       "3             spacy_similarity  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_index</th>\n",
       "      <th>spacy_similarity_match_index</th>\n",
       "      <th>spacy_similarity</th>\n",
       "      <th>closest_similar_index</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5041</td>\n",
       "      <td>1.413947</td>\n",
       "      <td>5041</td>\n",
       "      <td>1.413947</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.481956</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.481956</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c",
    "ExecuteTime": {
     "end_time": "2024-11-22T14:13:55.045307Z",
     "start_time": "2024-11-22T14:13:54.977301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract index from target (dictionary) and reset index\n",
    "reset_dictionary_df = (dictionary_df.\n",
    "                       loc[matches_df['target_index']].\n",
    "                       reset_index(drop=True))\n",
    "\n",
    "# extract index from proposed match (acord) and reset index\n",
    "reset_acord_df = (acord_df.\n",
    "                  loc[matches_df['closest_similar_index']].\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "# join target df with matches df\n",
    "joined_df = reset_dictionary_df.join(reset_acord_df)\n",
    "\n",
    "# append similarity score\n",
    "def stripHtml (html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "    \n",
    "joined_df['closest_similarity_score'] = matches_df['closest_similarity_score']\n",
    "joined_df['closest_similarity_algorithm'] = matches_df['closest_similarity_algorithm']\n",
    "\n",
    "joined_df['Definition'] =  (\n",
    "    joined_df['Definition'].apply(stripHtml))\n",
    "\n",
    "joined_df = joined_df[[\n",
    "    'field_names',\n",
    "    'Glossary Terms',\n",
    "    'Definition',\n",
    "    'closest_similarity_score',\n",
    "    'closest_similarity_algorithm'\n",
    "]]\n",
    "\n",
    "joined_df.to_csv('test-data/out.csv', index=False)\n",
    "joined_df"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_18268\\3526957015.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      field_names            Glossary Terms  \\\n",
       "0  POLICY_NUMBERs     Property Improvements   \n",
       "1      POL_NUMBER             Policy Number   \n",
       "2        COVERAGE                  Coverage   \n",
       "3  ANNUAL_PREMIUM  Guideline Annual Premium   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  Permanent additions or changes made to a build...   \n",
       "1  A unique identifier assigned to a policy (e.g....   \n",
       "2  A financial services agreement component detai...   \n",
       "3  This is the premium that needs to be paid for ...   \n",
       "\n",
       "   closest_similarity_score closest_similarity_algorithm  \n",
       "0                  1.413947             spacy_similarity  \n",
       "1                  0.000000             spacy_similarity  \n",
       "2                  0.000000             spacy_similarity  \n",
       "3                  1.481956             spacy_similarity  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>Glossary Terms</th>\n",
       "      <th>Definition</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>Property Improvements</td>\n",
       "      <td>Permanent additions or changes made to a build...</td>\n",
       "      <td>1.413947</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>Coverage</td>\n",
       "      <td>A financial services agreement component detai...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>Guideline Annual Premium</td>\n",
       "      <td>This is the premium that needs to be paid for ...</td>\n",
       "      <td>1.481956</td>\n",
       "      <td>spacy_similarity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b50cc05637aa6576",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
