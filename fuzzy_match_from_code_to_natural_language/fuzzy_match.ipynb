{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:21.784754Z",
     "start_time": "2024-11-09T10:38:21.779334Z"
    },
    "id": "initial_id"
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "\n",
    "import pandas\n",
    "import numpy as np\n",
    "import re\n",
    "from itertools import product\n",
    "from nltk.metrics import edit_distance"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:18:57.378040Z",
     "start_time": "2024-11-09T11:18:57.373533Z"
    },
    "id": "7717da458e217594"
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def variable_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # tokens = []\n",
    "    # for token in snake_tokenize(string):\n",
    "    #     for sub_token in space_tokenize(token):\n",
    "    #         tokens.extend(variable_tokenize(sub_token))\n",
    "    tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:19:01.379484Z",
     "start_time": "2024-11-09T11:19:01.373497Z"
    },
    "id": "4f700e12fd4994aa"
   },
   "cell_type": "code",
   "source": [
    "token_replacements = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def replace_common_abbreviated_tokens(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in token_replacements:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def clean_and_tokenize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = replace_common_abbreviated_tokens(tokens)\n",
    "    return tokens\n"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:19:25.039753Z",
     "start_time": "2024-11-09T11:19:25.031246Z"
    },
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366"
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Make a fake data dictionary for testing\n",
    "\"\"\"\n",
    "dictionary_fields = [\n",
    "    \"POLICY_NUMBERs\",\n",
    "    \"POL_NUMBER\",\n",
    "    # \"policyNumber\",\n",
    "    # \"polNo\",\n",
    "    # \"PolicyNumber\",\n",
    "    \"COVERAGE\",\n",
    "    \"ANNUAL_PREMIUM\"\n",
    "]\n",
    "\n",
    "dictionary_df = pandas.DataFrame(\n",
    "    dictionary_fields,\n",
    "    columns=[\"field_names\"])\n",
    "\n",
    "dictionary_df['tokenized_field_names'] = (\n",
    "    dictionary_df['field_names'].apply(clean_and_tokenize))\n",
    "\n",
    "dictionary_df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      field_names tokenized_field_names\n",
       "0  POLICY_NUMBERs     [policy, numbers]\n",
       "1      POL_NUMBER      [policy, number]\n",
       "2        COVERAGE            [coverage]\n",
       "3  ANNUAL_PREMIUM     [annual, premium]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>tokenized_field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>[policy, numbers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>[policy, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>[coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>[annual, premium]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:21.962272Z",
     "start_time": "2024-11-09T10:38:21.900821Z"
    },
    "id": "8e97c24098aa98c1"
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Read the glossary data\n",
    "\"\"\"\n",
    "acord_df = pandas.read_csv(\n",
    "    'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "    header=0)\n",
    "\n",
    "acord_df['tokenized_glossary'] = (\n",
    "    acord_df['Glossary Terms'].apply(clean_and_tokenize))\n",
    "\n",
    "# print(acord_df['tokenized_glossary'].head())"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:21.988935Z",
     "start_time": "2024-11-09T10:38:21.983969Z"
    },
    "id": "f3df8d63c056072e"
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "attempts to handle out of order words in each token list.\n",
    "\"\"\"\n",
    "def source_token_within_search_edit_distance(tokenized_term1, tokenized_term2):\n",
    "\n",
    "    # get the best matched tokens from a list of tuples\n",
    "    #   each tuple has a \"target\" token, a \"potential match\" token, and a similarity score\n",
    "    def best_matches(tuples_list):\n",
    "\n",
    "        best = {}\n",
    "        for target, potential_match, score in tuples_list:\n",
    "            # Check if we have seen this target before or if the current score is better\n",
    "            if target not in best or score < best[target][2]:\n",
    "                best[target] = (target, potential_match, score)\n",
    "\n",
    "        return list(best.values())\n",
    "\n",
    "\n",
    "    # make unique pairs\n",
    "    l1 = list(set(tokenized_term1))\n",
    "    l2 = list(set(tokenized_term2))\n",
    "    pairs = product(l1, l2)\n",
    "\n",
    "    # calculate the Jaccard distance between all pairs\n",
    "    token_distances = [(token1, token2, edit_distance(token1, token2))\n",
    "                       for token1, token2 in pairs]\n",
    "\n",
    "\n",
    "    best = best_matches(token_distances)\n",
    "    # todo: the total distance needs to handle the \"extra\" fields in each token list that are not \"best matches\"\n",
    "    #    e.g., \"policy number\" and \"the policy number\" has an extra \"the\" in the second list of tokens.\n",
    "    #    and vice versa\n",
    "    total_distance = sum(token_distance[2] for token_distance in best)\n",
    "\n",
    "    if len(tokenized_term2) - len(tokenized_term1) > 0:\n",
    "        extra_terms = len(tokenized_term2) - len(tokenized_term1)\n",
    "        extra_term_penalty = extra_terms + (extra_terms * total_distance)\n",
    "        # extra_term_penalty = 0\n",
    "    else:\n",
    "        extra_term_penalty = 0\n",
    "\n",
    "    return total_distance + extra_term_penalty"
   ],
   "id": "f3df8d63c056072e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:22.023350Z",
     "start_time": "2024-11-09T10:38:22.019630Z"
    },
    "id": "d9f1c3b665797f44"
   },
   "cell_type": "code",
   "source": [
    "def ordered_token_edit_distance(tokenized_term1, tokenized_term2):\n",
    "    total_distance = 0\n",
    "\n",
    "    if len(tokenized_term1) == len(tokenized_term2):\n",
    "        # modification on pure edit distance of the entire token list: if the lists are the same length\n",
    "        #    then discount the distance when two tokens start with the same sequence\n",
    "        #    for example: `policy` and `form` are the same distance from `pol` but `pol` is far\n",
    "        #    more likely to be closer to `policy`\n",
    "        for (token1, token2) in zip(tokenized_term1, tokenized_term2):\n",
    "            my_distance = edit_distance(token1, token2)\n",
    "            my_distance = my_distance / 2 if token2.startswith(token1) else my_distance\n",
    "            total_distance = total_distance + my_distance\n",
    "    else:\n",
    "        # otherwise join the lists back together with spaces (to preserve `token differentiation`)\n",
    "        #    and edit distance those strings\n",
    "        space = \" \"\n",
    "        string1 = space.join(tokenized_term1).strip()\n",
    "        string2 = space.join(tokenized_term2).strip()\n",
    "        total_distance = edit_distance(string1, string2)\n",
    "\n",
    "    return total_distance"
   ],
   "id": "d9f1c3b665797f44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:57:47.053074Z",
     "start_time": "2024-11-09T10:57:47.047160Z"
    },
    "id": "f12016d8f5e2661e",
    "outputId": "6e86c791-b588-4fe8-dda1-bf6d12b5305e"
   },
   "cell_type": "code",
   "source": [
    "print(\"a\", ordered_token_edit_distance(clean_and_tokenize(\"POL_NUMBER\"), clean_and_tokenize(\"Policy Number\")))\n",
    "print(\"b\", ordered_token_edit_distance(clean_and_tokenize(\"POL_NUMBER\"), clean_and_tokenize(\"From Number\")))\n",
    "print(\"c\", ordered_token_edit_distance(clean_and_tokenize(\"POL_NUMBER\"), clean_and_tokenize(\"The Policy Number\")))\n",
    "print(\"d\", ordered_token_edit_distance(clean_and_tokenize(\"POL_NUMBER\"), clean_and_tokenize(\"The Form Number\")))"
   ],
   "id": "f12016d8f5e2661e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0\n",
      "b 6.0\n",
      "c 4\n",
      "d 8\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:22.069812Z",
     "start_time": "2024-11-09T10:38:22.065646Z"
    },
    "id": "e086e12a4fe45428"
   },
   "cell_type": "code",
   "source": [
    "def find_closest_match(target_df=None, target_field=None, match_df=None, match_field=None, match_algorithm=None, match_score_field=None, matches_df=None):\n",
    "    # Prepare an empty list to store closest matches\n",
    "    closest_matches = []\n",
    "\n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, dict_tokens in target_df[target_field].items():\n",
    "        # print('> ', idx, dict_tokens)\n",
    "        best_similarity = float('inf')\n",
    "        best_match_idx = None\n",
    "\n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, acord_tokens in match_df[match_field].items():\n",
    "            similarity = match_algorithm(dict_tokens, acord_tokens)\n",
    "            # print('> ', similarity, best_similarity, dict_tokens, acord_tokens)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "\n",
    "        # Append the best match for the current dictionary token\n",
    "        closest_matches.append([idx, best_match_idx, best_similarity])\n",
    "\n",
    "    return pandas.DataFrame(closest_matches, columns=[\n",
    "        \"target_index\",\n",
    "        match_score_field + \"_match_index\",\n",
    "        match_score_field\n",
    "    ])"
   ],
   "id": "e086e12a4fe45428",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T11:21:04.093619Z",
     "start_time": "2024-11-09T11:20:58.527173Z"
    },
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5"
   },
   "cell_type": "code",
   "source": [
    "match_algorithms = [\n",
    "    (\"similarity\", ordered_token_edit_distance),\n",
    "    (\"best_tokens_similarity\", source_token_within_search_edit_distance),\n",
    "    (\"similarity3\", ordered_token_edit_distance),\n",
    "]\n",
    "\n",
    "matches_df = None\n",
    "for match_score_field, match_algorithm in match_algorithms:\n",
    "    # Call the `find_closest_match` function\n",
    "    my_matches_df = find_closest_match(\n",
    "        target_df=dictionary_df,\n",
    "        target_field='tokenized_field_names',\n",
    "        match_df=acord_df,\n",
    "        match_field='tokenized_glossary',\n",
    "        match_algorithm=match_algorithm,\n",
    "        match_score_field=match_score_field\n",
    "    )\n",
    "\n",
    "    # If final_df is None, set it to matches_df\n",
    "    if matches_df is None:\n",
    "        matches_df = my_matches_df\n",
    "    else:\n",
    "        matches_df = matches_df.merge(my_matches_df, on='target_index')\n",
    "\n",
    "\n",
    "# best_score_field, best_algo = max(match_algorithms, key=lambda x: matches_df[x[0]].max())\n",
    "\n",
    "match_metadata = [\n",
    "    (score_field, score_field + '_match_index', algo.__name__)\n",
    "    for score_field, algo in match_algorithms\n",
    "]\n",
    "\n",
    "print(match_metadata)\n",
    "matches_df['closest_similar_index'] = np.where(\n",
    "    matches_df['similarity'] > matches_df['best_tokens_similarity'],\n",
    "    matches_df['best_tokens_similarity_match_index'],\n",
    "    matches_df['similarity_match_index']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_score'] = np.where(\n",
    "    matches_df['similarity'] > matches_df['best_tokens_similarity'],\n",
    "    matches_df['best_tokens_similarity'],\n",
    "    matches_df['similarity']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_algorithm'] = np.where(\n",
    "    matches_df['similarity'] > matches_df['best_tokens_similarity'],\n",
    "    'source_token_within_search_edit_distance',\n",
    "    'ordered_token_edit_distance'\n",
    ")\n",
    "# print (source_token_within_search_edit_distance.__name__)\n",
    "\n",
    "# matches_df"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('similarity', 'similarity_match_index', 'ordered_token_edit_distance'), ('best_tokens_similarity', 'best_tokens_similarity_match_index', 'source_token_within_search_edit_distance'), ('similarity3', 'similarity3_match_index', 'ordered_token_edit_distance')]\n",
      "similarity\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T10:38:55.476112Z",
     "start_time": "2024-11-09T10:38:55.467262Z"
    },
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c"
   },
   "cell_type": "code",
   "source": [
    "# extract index from target (dictionary) and reset index\n",
    "reset_dictionary_df = (dictionary_df.\n",
    "                       loc[matches_df['target_index']].\n",
    "                       reset_index(drop=True))\n",
    "\n",
    "# extract index from proposed match (acord) and reset index\n",
    "reset_acord_df = (acord_df.\n",
    "                  loc[matches_df['closest_similar_index']].\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "# join target df with matches df\n",
    "joined_df = reset_dictionary_df.join(reset_acord_df)\n",
    "\n",
    "# append similarity score\n",
    "joined_df['closest_similarity_score'] = matches_df['closest_similarity_score']\n",
    "joined_df['closest_similarity_algorithm'] = matches_df['closest_similarity_algorithm']\n",
    "\n",
    "joined_df = joined_df[[\n",
    "    'field_names',\n",
    "    'Glossary Terms',\n",
    "    # 'Definition',\n",
    "    'closest_similarity_score',\n",
    "    'closest_similarity_algorithm'\n",
    "]]\n",
    "\n",
    "print(joined_df.head(10))\n",
    "joined_df.to_csv('test-data/out.csv', index=False)"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      field_names            Glossary Terms  closest_similarity_score  \\\n",
      "0  POLICY_NUMBERs             Policy Number                       2.0   \n",
      "1      POL_NUMBER             Policy Number                       0.0   \n",
      "2    policyNumber             Policy Number                       0.0   \n",
      "3           polNo             Policy Number                       0.0   \n",
      "4    PolicyNumber             Policy Number                       0.0   \n",
      "5        COVERAGE                  Coverage                       0.0   \n",
      "6  ANNUAL_PREMIUM  Guideline Annual Premium                       1.0   \n",
      "\n",
      "               closest_similarity_algorithm  \n",
      "0               ordered_token_edit_distance  \n",
      "1               ordered_token_edit_distance  \n",
      "2               ordered_token_edit_distance  \n",
      "3               ordered_token_edit_distance  \n",
      "4               ordered_token_edit_distance  \n",
      "5               ordered_token_edit_distance  \n",
      "6  source_token_within_search_edit_distance  \n"
     ]
    }
   ],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
