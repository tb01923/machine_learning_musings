{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:39:30.455195Z",
     "start_time": "2024-11-21T19:39:30.451589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tkinter.font import names\n",
    "import re\n",
    "import pandas\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from nltk.metrics import edit_distance\n",
    "from gensim.models import Word2Vec\n",
    "from bs4 import BeautifulSoup #pip install beautifulsoup4\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "id": "7717da458e217594",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:27:18.427020Z",
     "start_time": "2024-11-21T19:27:18.423217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "snake_tokenize = lambda string: re.split(r'[_]', string)\n",
    "space_tokenize = lambda string: re.split(r'[ ]', string)\n",
    "snake_space_tokenize = lambda string: re.split(r'[ _]', string)\n",
    "\n",
    "def camel_case_tokenize(string):\n",
    "    # This regex pattern will split at the transitions between lowercase and uppercase letters\n",
    "    pattern = r'(?<=[a-z])(?=[A-Z])|(?<!^)(?=[A-Z][a-z])'\n",
    "\n",
    "    # Use re.split to split the string based on the pattern\n",
    "    tokens = re.split(pattern, string)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize(string):\n",
    "    # not needed right now:\n",
    "    # -----------------------------------------------------\n",
    "    # tokens = []\n",
    "    # for token in snake_tokenize(string):\n",
    "    #     for sub_token in space_tokenize(token):\n",
    "    #         tokens.extend(variable_tokenize(sub_token))\n",
    "    tokens = snake_space_tokenize(string)\n",
    "    return tokens"
   ],
   "id": "7717da458e217594",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "id": "4f700e12fd4994aa",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:27:19.043759Z",
     "start_time": "2024-11-21T19:27:19.039093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "replacement_tokens = [\n",
    "    ('pol', 'policy'),\n",
    "    ('plcy', 'policy'),\n",
    "    ('no', 'number'),\n",
    "    ('cov', 'coverage')\n",
    "]\n",
    "\n",
    "def clean(string):\n",
    "    return string.replace('\"', '').lower()\n",
    "\n",
    "def normalize(tokens):\n",
    "    def replace_token(token):\n",
    "        for old, new in replacement_tokens:\n",
    "            if token == old:\n",
    "                return new\n",
    "        return token\n",
    "    return [replace_token(token) for token in tokens]\n",
    "\n",
    "def tokenize_then_normalize(string):\n",
    "    # tokenize first for variable tokens\n",
    "    tokens = tokenize(string)\n",
    "    # clean each token, convert case\n",
    "    tokens = list(map(clean, tokens))\n",
    "    # perform common replacements\n",
    "    tokens = normalize(tokens)\n",
    "    return tokens\n"
   ],
   "id": "4f700e12fd4994aa",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "id": "bbf496ef6193329",
    "outputId": "07a51bf4-d897-43b2-e450-2987ebccd366",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:45:26.120014Z",
     "start_time": "2024-11-21T19:45:26.111570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_df(df, field, result_prefix=None):\n",
    "    if result_prefix is None:\n",
    "        df['tokenized_' + field] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "    else:\n",
    "        df[result_prefix] = (\n",
    "            df[field].apply(tokenize_then_normalize))\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Make a fake data dictionary for testing\n",
    "\"\"\"\n",
    "dictionary_fields = [\n",
    "    \"POLICY_NUMBERs\",\n",
    "    \"POL_NUMBER\",\n",
    "    # \"policyNumber\",\n",
    "    # \"polNo\",\n",
    "    # \"PolicyNumber\",\n",
    "    \"COVERAGE\",\n",
    "    \"ANNUAL_PREMIUM\"\n",
    "]\n",
    "\n",
    "dictionary_df = pandas.DataFrame(\n",
    "    dictionary_fields,\n",
    "    columns=[\"field_names\"])\n",
    "\n",
    "dictionary_df = prepare_df(dictionary_df, \"field_names\")\n",
    "dictionary_df"
   ],
   "id": "bbf496ef6193329",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      field_names tokenized_field_names\n",
       "0  POLICY_NUMBERs     [policy, numbers]\n",
       "1      POL_NUMBER      [policy, number]\n",
       "2        COVERAGE            [coverage]\n",
       "3  ANNUAL_PREMIUM     [annual, premium]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>tokenized_field_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>[policy, numbers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>[policy, number]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>[coverage]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>[annual, premium]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "id": "8e97c24098aa98c1",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:45:32.011757Z",
     "start_time": "2024-11-21T19:45:31.951281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Read the glossary data\n",
    "\"\"\"\n",
    "acord_df = pandas.read_csv(\n",
    "    'test-data/ACORD-Business-Glossary Model 2.13.csv',\n",
    "    header=0)\n",
    "\n",
    "acord_df = prepare_df(acord_df, \"Glossary Terms\", 'tokenized_glossary') \n",
    "\n",
    "acord_df[['Glossary Terms', 'tokenized_glossary']]"
   ],
   "id": "8e97c24098aa98c1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                Glossary Terms  \\\n",
       "0                                    A\" rates\"   \n",
       "1                              A I Or Robotics   \n",
       "2                                          A&E   \n",
       "3                   A-Share Variable Annuities   \n",
       "4                             A.M. Best rating   \n",
       "...                                        ...   \n",
       "6227                                      eEg7   \n",
       "6228  excess and surplus (E&S) lines insurance   \n",
       "6229                           fringe benefits   \n",
       "6230                                      hSOD   \n",
       "6231      tobacco Cessation Program Indicator    \n",
       "\n",
       "                                   tokenized_glossary  \n",
       "0                                          [a, rates]  \n",
       "1                                [a, i, or, robotics]  \n",
       "2                                               [a&e]  \n",
       "3                      [a-share, variable, annuities]  \n",
       "4                                [a.m., best, rating]  \n",
       "...                                               ...  \n",
       "6227                                           [eeg7]  \n",
       "6228  [excess, and, surplus, (e&s), lines, insurance]  \n",
       "6229                               [fringe, benefits]  \n",
       "6230                                           [hsod]  \n",
       "6231       [tobacco, cessation, program, indicator, ]  \n",
       "\n",
       "[6232 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Glossary Terms</th>\n",
       "      <th>tokenized_glossary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A\" rates\"</td>\n",
       "      <td>[a, rates]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A I Or Robotics</td>\n",
       "      <td>[a, i, or, robotics]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A&amp;E</td>\n",
       "      <td>[a&amp;e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-Share Variable Annuities</td>\n",
       "      <td>[a-share, variable, annuities]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A.M. Best rating</td>\n",
       "      <td>[a.m., best, rating]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6227</th>\n",
       "      <td>eEg7</td>\n",
       "      <td>[eeg7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6228</th>\n",
       "      <td>excess and surplus (E&amp;S) lines insurance</td>\n",
       "      <td>[excess, and, surplus, (e&amp;s), lines, insurance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>fringe benefits</td>\n",
       "      <td>[fringe, benefits]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6230</th>\n",
       "      <td>hSOD</td>\n",
       "      <td>[hsod]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6231</th>\n",
       "      <td>tobacco Cessation Program Indicator</td>\n",
       "      <td>[tobacco, cessation, program, indicator, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6232 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "id": "f3df8d63c056072e",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:28:14.892299Z",
     "start_time": "2024-11-21T19:28:14.886712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "attempts to handle out of order words in each token list.\n",
    "\"\"\"\n",
    "def best_paired_tokens_edit_distance(tokenized_term1, tokenized_term2):\n",
    "\n",
    "    # get the best matched tokens from a list of tuples\n",
    "    #   each tuple has a \"target\" token, a \"potential match\" token, and a similarity score\n",
    "    def best_matches(tuples_list):\n",
    "\n",
    "        best = {}\n",
    "        for target, potential_match, score in tuples_list:\n",
    "            # Check if we have seen this target before or if the current score is better\n",
    "            if target not in best or score < best[target][2]:\n",
    "                best[target] = (target, potential_match, score)\n",
    "\n",
    "        return list(best.values())\n",
    "\n",
    "\n",
    "    # make unique pairs\n",
    "    l1 = list(set(tokenized_term1))\n",
    "    l2 = list(set(tokenized_term2))\n",
    "    pairs = product(l1, l2)\n",
    "\n",
    "    # calculate the Jaccard distance between all pairs\n",
    "    token_distances = [(token1, token2, edit_distance(token1, token2))\n",
    "                       for token1, token2 in pairs]\n",
    "\n",
    "\n",
    "    best = best_matches(token_distances)\n",
    "    # todo: the total distance needs to handle the \"extra\" fields in each token list that are not \"best matches\"\n",
    "    #    e.g., \"policy number\" and \"the policy number\" has an extra \"the\" in the second list of tokens.\n",
    "    #    and vice versa\n",
    "    total_distance = sum(token_distance[2] for token_distance in best)\n",
    "\n",
    "    if len(tokenized_term2) - len(tokenized_term1) > 0:\n",
    "        extra_terms = len(tokenized_term2) - len(tokenized_term1)\n",
    "        extra_term_penalty = extra_terms + (extra_terms * total_distance)\n",
    "        # extra_term_penalty = 0\n",
    "    else:\n",
    "        extra_term_penalty = 0\n",
    "\n",
    "    return total_distance + extra_term_penalty"
   ],
   "id": "f3df8d63c056072e",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "id": "d9f1c3b665797f44",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:28:16.017843Z",
     "start_time": "2024-11-21T19:28:16.013236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def modified_edit_distance(tokenized_term1, tokenized_term2):\n",
    "    total_distance = 0\n",
    "\n",
    "    if len(tokenized_term1) == len(tokenized_term2):\n",
    "        # modification on pure edit distance of the entire token list: if the lists are the same length\n",
    "        #    then discount the distance when two tokens start with the same sequence\n",
    "        #    for example: `policy` and `form` are the same distance from `pol` but `pol` is far\n",
    "        #    more likely to be closer to `policy`\n",
    "        for (token1, token2) in zip(tokenized_term1, tokenized_term2):\n",
    "            my_distance = edit_distance(token1, token2)\n",
    "            my_distance = my_distance / 2 if token2.startswith(token1) else my_distance\n",
    "            total_distance = total_distance + my_distance\n",
    "    else:\n",
    "        # otherwise join the lists back together with spaces (to preserve `token differentiation`)\n",
    "        #    and edit distance those strings\n",
    "        space = \" \"\n",
    "        string1 = space.join(tokenized_term1).strip()\n",
    "        string2 = space.join(tokenized_term2).strip()\n",
    "        total_distance = edit_distance(string1, string2)\n",
    "\n",
    "    return total_distance"
   ],
   "id": "d9f1c3b665797f44",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "id": "f12016d8f5e2661e",
    "outputId": "6e86c791-b588-4fe8-dda1-bf6d12b5305e",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:28:16.925450Z",
     "start_time": "2024-11-21T19:28:16.922126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"a\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"Policy Number\")))\n",
    "print(\"b\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"From Number\")))\n",
    "print(\"c\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Policy Number\")))\n",
    "print(\"d\", modified_edit_distance(tokenize_then_normalize(\"POL_NUMBER\"), tokenize_then_normalize(\"The Form Number\")))"
   ],
   "id": "f12016d8f5e2661e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 0.0\n",
      "b 6.0\n",
      "c 4\n",
      "d 8\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "id": "e086e12a4fe45428",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:28:17.880233Z",
     "start_time": "2024-11-21T19:28:17.876111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def find_closest_match(target_df=None, target_field=None, match_df=None, match_field=None, match_algorithm=None, match_score_field=None, matches_df=None):\n",
    "    # Prepare an empty list to store closest matches\n",
    "    closest_matches = []\n",
    "\n",
    "    # Iterate over each tokenized field name in dictionary_df\n",
    "    for idx, dict_tokens in target_df[target_field].items():\n",
    "        # print('> ', idx, dict_tokens)\n",
    "        best_similarity = float('inf')\n",
    "        best_match_idx = None\n",
    "\n",
    "        # Compare with each tokenized glossary term in acord_df\n",
    "        for a_idx, acord_tokens in match_df[match_field].items():\n",
    "            similarity = match_algorithm(dict_tokens, acord_tokens)\n",
    "            # print('> ', similarity, best_similarity, dict_tokens, acord_tokens)\n",
    "            if similarity < best_similarity:\n",
    "                best_similarity = similarity\n",
    "                best_match_idx = a_idx\n",
    "                if best_similarity == 0:\n",
    "                    break\n",
    "\n",
    "        # Append the best match for the current dictionary token\n",
    "        closest_matches.append([idx, best_match_idx, best_similarity])\n",
    "\n",
    "    return pandas.DataFrame(closest_matches, columns=[\n",
    "        \"target_index\",\n",
    "        match_score_field + \"_match_index\",\n",
    "        match_score_field\n",
    "    ])"
   ],
   "id": "e086e12a4fe45428",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "id": "f2414d1d8839fa0f",
    "outputId": "ee758d61-a745-4076-cde6-cdca6cda91a5",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:29:19.604119Z",
     "start_time": "2024-11-21T19:29:15.819722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "match_algorithms = [\n",
    "    (\"modified_edit_distance\", modified_edit_distance),\n",
    "    (\"best_paired_tokens_edit_distance\", best_paired_tokens_edit_distance),\n",
    "    # (\"similarity3\", modified_edit_distance),\n",
    "]\n",
    "\n",
    "matches_df = None\n",
    "for match_score_field, match_algorithm in match_algorithms:\n",
    "    # Call the `find_closest_match` function\n",
    "    my_matches_df = find_closest_match(\n",
    "        target_df=dictionary_df,\n",
    "        target_field='tokenized_field_names',\n",
    "        match_df=acord_df,\n",
    "        match_field='tokenized_glossary',\n",
    "        match_algorithm=match_algorithm,\n",
    "        match_score_field=match_score_field\n",
    "    )\n",
    "\n",
    "    # If final_df is None, set it to matches_df\n",
    "    if matches_df is None:\n",
    "        matches_df = my_matches_df\n",
    "    else:\n",
    "        matches_df = matches_df.merge(my_matches_df, on='target_index')\n",
    "\n",
    "\n",
    "# best_score_field, best_algo = max(match_algorithms, key=lambda x: matches_df[x[0]].max())\n",
    "\n",
    "match_metadata = [\n",
    "    (score_field, score_field + '_match_index', algo.__name__)\n",
    "    for score_field, algo in match_algorithms\n",
    "]\n",
    "\n",
    "# print(match_metadata)\n",
    "matches_df['closest_similar_index'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['best_paired_tokens_edit_distance_match_index'],\n",
    "    matches_df['modified_edit_distance_match_index']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_score'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['best_paired_tokens_edit_distance'],\n",
    "    matches_df['modified_edit_distance']\n",
    ")\n",
    "\n",
    "matches_df['closest_similarity_algorithm'] = np.where(\n",
    "    matches_df['modified_edit_distance'] > matches_df['best_paired_tokens_edit_distance'],\n",
    "    'best_paired_tokens_edit_distance',\n",
    "    'modified_edit_distance'\n",
    ")\n",
    "# print (source_token_within_search_edit_distance.__name__)\n",
    "\n",
    "matches_df"
   ],
   "id": "f2414d1d8839fa0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   target_index  modified_edit_distance_match_index  modified_edit_distance  \\\n",
       "0             0                                4811                     1.0   \n",
       "1             1                                4811                     0.0   \n",
       "2             2                                1649                     0.0   \n",
       "3             3                                2222                     5.0   \n",
       "\n",
       "   best_paired_tokens_edit_distance_match_index  \\\n",
       "0                                          4811   \n",
       "1                                          4811   \n",
       "2                                          1649   \n",
       "3                                          3021   \n",
       "\n",
       "   best_paired_tokens_edit_distance  closest_similar_index  \\\n",
       "0                                 1                   4811   \n",
       "1                                 0                   4811   \n",
       "2                                 0                   1649   \n",
       "3                                 1                   3021   \n",
       "\n",
       "   closest_similarity_score      closest_similarity_algorithm  \n",
       "0                       1.0            modified_edit_distance  \n",
       "1                       0.0            modified_edit_distance  \n",
       "2                       0.0            modified_edit_distance  \n",
       "3                       1.0  best_paired_tokens_edit_distance  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_index</th>\n",
       "      <th>modified_edit_distance_match_index</th>\n",
       "      <th>modified_edit_distance</th>\n",
       "      <th>best_paired_tokens_edit_distance_match_index</th>\n",
       "      <th>best_paired_tokens_edit_distance</th>\n",
       "      <th>closest_similar_index</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4811</td>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>1.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4811</td>\n",
       "      <td>0</td>\n",
       "      <td>4811</td>\n",
       "      <td>0.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1649</td>\n",
       "      <td>0</td>\n",
       "      <td>1649</td>\n",
       "      <td>0.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2222</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3021</td>\n",
       "      <td>1</td>\n",
       "      <td>3021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>best_paired_tokens_edit_distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "id": "99571a1ddaeb8cd6",
    "outputId": "c9491be5-7110-4ae2-81fb-be500d2d429c",
    "ExecuteTime": {
     "end_time": "2024-11-21T19:41:14.271293Z",
     "start_time": "2024-11-21T19:41:14.258468Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# extract index from target (dictionary) and reset index\n",
    "reset_dictionary_df = (dictionary_df.\n",
    "                       loc[matches_df['target_index']].\n",
    "                       reset_index(drop=True))\n",
    "\n",
    "# extract index from proposed match (acord) and reset index\n",
    "reset_acord_df = (acord_df.\n",
    "                  loc[matches_df['closest_similar_index']].\n",
    "                  reset_index(drop=True))\n",
    "\n",
    "# join target df with matches df\n",
    "joined_df = reset_dictionary_df.join(reset_acord_df)\n",
    "\n",
    "# append similarity score\n",
    "def stripHtml (html):\n",
    "    soup = BeautifulSoup(html)\n",
    "    text = soup.get_text()\n",
    "    text = text.replace('\\n', '')\n",
    "    return text\n",
    "    \n",
    "joined_df['closest_similarity_score'] = matches_df['closest_similarity_score']\n",
    "joined_df['closest_similarity_algorithm'] = matches_df['closest_similarity_algorithm']\n",
    "\n",
    "joined_df['Definition'] =  (\n",
    "    joined_df['Definition'].apply(stripHtml))\n",
    "\n",
    "joined_df = joined_df[[\n",
    "    'field_names',\n",
    "    'Glossary Terms',\n",
    "    'Definition',\n",
    "    'closest_similarity_score',\n",
    "    'closest_similarity_algorithm'\n",
    "]]\n",
    "\n",
    "joined_df.to_csv('test-data/out.csv', index=False)\n",
    "joined_df"
   ],
   "id": "99571a1ddaeb8cd6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tbadmin\\AppData\\Local\\Temp\\ipykernel_21368\\3526957015.py:16: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(html)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      field_names            Glossary Terms  \\\n",
       "0  POLICY_NUMBERs             Policy Number   \n",
       "1      POL_NUMBER             Policy Number   \n",
       "2        COVERAGE                  Coverage   \n",
       "3  ANNUAL_PREMIUM  Guideline Annual Premium   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  A unique identifier assigned to a policy (e.g....   \n",
       "1  A unique identifier assigned to a policy (e.g....   \n",
       "2  A financial services agreement component detai...   \n",
       "3  This is the premium that needs to be paid for ...   \n",
       "\n",
       "   closest_similarity_score      closest_similarity_algorithm  \n",
       "0                       1.0            modified_edit_distance  \n",
       "1                       0.0            modified_edit_distance  \n",
       "2                       0.0            modified_edit_distance  \n",
       "3                       1.0  best_paired_tokens_edit_distance  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_names</th>\n",
       "      <th>Glossary Terms</th>\n",
       "      <th>Definition</th>\n",
       "      <th>closest_similarity_score</th>\n",
       "      <th>closest_similarity_algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLICY_NUMBERs</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>1.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POL_NUMBER</td>\n",
       "      <td>Policy Number</td>\n",
       "      <td>A unique identifier assigned to a policy (e.g....</td>\n",
       "      <td>0.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVERAGE</td>\n",
       "      <td>Coverage</td>\n",
       "      <td>A financial services agreement component detai...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>modified_edit_distance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANNUAL_PREMIUM</td>\n",
       "      <td>Guideline Annual Premium</td>\n",
       "      <td>This is the premium that needs to be paid for ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>best_paired_tokens_edit_distance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b50cc05637aa6576"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "include_colab_link": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
